{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F5dXu5a4-hno"
   },
   "outputs": [],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f6Dur-1-pGwq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "id": "VQCMHg8HoLug",
    "outputId": "46bbf4a1-e2e5-47ee-f4ce-0593d7b45b34"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Sentiment Model Evaluation on SQLite Database\n",
    "Evaluates a HuggingFace sentiment model on 1000 random reviews\n",
    "\"\"\"\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "\n",
    "def balance_dataset(df, method='undersample', sentiment_col='voted_up'):\n",
    "    \"\"\"Balance the dataset by sentiment\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"DATASET BALANCING (Method: {method.upper()})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get counts\n",
    "    sentiment_counts = df[sentiment_col].value_counts()\n",
    "    print(f\"\\nBefore balancing:\")\n",
    "    for label, count in sentiment_counts.items():\n",
    "        print(f\"  {label}: {count:,}\")\n",
    "\n",
    "    if method == 'undersample':\n",
    "        # Undersample majority class\n",
    "        min_count = sentiment_counts.min()\n",
    "\n",
    "        balanced_dfs = []\n",
    "        for label in sentiment_counts.index:\n",
    "            label_df = df[df[sentiment_col] == label]\n",
    "            sampled_df = label_df.sample(n=min_count, random_state=42)\n",
    "            balanced_dfs.append(sampled_df)\n",
    "\n",
    "        df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "    elif method == 'oversample':\n",
    "        # Oversample minority class\n",
    "        max_count = sentiment_counts.max()\n",
    "\n",
    "        balanced_dfs = []\n",
    "        for label in sentiment_counts.index:\n",
    "            label_df = df[df[sentiment_col] == label]\n",
    "            sampled_df = label_df.sample(n=max_count, replace=True, random_state=42)\n",
    "            balanced_dfs.append(sampled_df)\n",
    "\n",
    "        df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid method! Using original dataset.\")\n",
    "        return df\n",
    "\n",
    "    # Shuffle\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Show new distribution\n",
    "    new_counts = df_balanced[sentiment_col].value_counts()\n",
    "    print(f\"\\nAfter balancing:\")\n",
    "    for label, count in new_counts.items():\n",
    "        print(f\"  {label}: {count:,}\")\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "\n",
    "def load_data_from_sqlite(db_path: str, text_column: str = 'review', label_column: str = 'voted_up', table_name: str = 'reviews', limit: int = 1000):\n",
    "    \"\"\"Load labeled reviews from SQLite database\"\"\"\n",
    "    print(\"Loading data from SQLite...\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Adjust column names according to your database schema\n",
    "    query = f\"\"\"\n",
    "        SELECT {text_column}, {label_column}\n",
    "        FROM {table_name}\n",
    "        WHERE {text_column} IS NOT NULL\n",
    "        AND {label_column} IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {limit}\"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"✓ Loaded {df.shape[0]} reviews\")\n",
    "    print(f\"  - Positive samples: {sum(df[label_column])}\")\n",
    "    print(f\"  - Negative samples: {len(df[label_column]) - sum(df[label_column])}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_sentiments(reviews, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                       batch_size=32):\n",
    "    \"\"\"\n",
    "    Predict sentiments using HuggingFace pipeline\n",
    "\n",
    "    Args:\n",
    "        reviews: List of review texts\n",
    "        model_name: HuggingFace model identifier\n",
    "        batch_size: Batch size for inference\n",
    "\n",
    "    Returns:\n",
    "        predictions: Binary predictions (0=negative, 1=positive)\n",
    "        probabilities: Confidence scores for positive class\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model_name,\n",
    "        device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    "    )\n",
    "\n",
    "    print(f\"Running inference on {len(reviews)} reviews...\")\n",
    "    results = sentiment_pipeline(reviews.to_list(), batch_size=batch_size,\n",
    "                                 truncation=True, max_length=512, padding=True)\n",
    "\n",
    "    # Convert to binary format (0=negative, 1=positive)\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "\n",
    "    for result in results:\n",
    "        # Handle different label formats\n",
    "        label = result['label'].upper()\n",
    "        score = result['score']\n",
    "\n",
    "        if 'POSITIVE' in label or label == 'LABEL_1' or label == '1' or label == 1:\n",
    "            predictions.append(1)\n",
    "            probabilities.append(score)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "            probabilities.append(1 - score)  # Flip probability for negative\n",
    "\n",
    "    print(\"✓ Predictions complete\")\n",
    "    return np.array(predictions), np.array(probabilities)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=['Negative', 'Positive']):\n",
    "    \"\"\"Plot confusion matrix heatmap\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_proba):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "             label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',\n",
    "             label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: roc_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_metrics_comparison(precision, recall, f1, class_names=['Negative', 'Positive']):\n",
    "    \"\"\"Plot bar chart comparing metrics per class\"\"\"\n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width, precision, width, label='Precision', color='steelblue')\n",
    "    bars2 = ax.bar(x, recall, width, label='Recall', color='coral')\n",
    "    bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='mediumseagreen')\n",
    "\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Per-Class Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: metrics_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_prediction_distribution(y_proba, y_true):\n",
    "    \"\"\"Plot distribution of prediction probabilities\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Histogram\n",
    "    ax1.hist(y_proba[y_true == 0], bins=30, alpha=0.6, label='Negative (True)', color='red')\n",
    "    ax1.hist(y_proba[y_true == 1], bins=30, alpha=0.6, label='Positive (True)', color='green')\n",
    "    ax1.set_xlabel('Predicted Probability (Positive Class)', fontsize=11)\n",
    "    ax1.set_ylabel('Frequency', fontsize=11)\n",
    "    ax1.set_title('Prediction Probability Distribution', fontsize=12, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # Box plot\n",
    "    data_to_plot = [y_proba[y_true == 0], y_proba[y_true == 1]]\n",
    "    box = ax2.boxplot(data_to_plot, labels=['Negative (True)', 'Positive (True)'],\n",
    "                      patch_artist=True)\n",
    "    box['boxes'][0].set_facecolor('lightcoral')\n",
    "    box['boxes'][1].set_facecolor('lightgreen')\n",
    "    ax2.set_ylabel('Predicted Probability (Positive Class)', fontsize=11)\n",
    "    ax2.set_title('Prediction Confidence by True Class', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: prediction_distribution.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba, class_names=['Negative', 'Positive']):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with all metrics and visualizations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SENTIMENT CLASSIFICATION EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None\n",
    "    )\n",
    "\n",
    "    # Macro and weighted averages\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "\n",
    "    # Additional metrics\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Print overall metrics\n",
    "    print(f\"\\n{'OVERALL METRICS':-^70}\")\n",
    "    print(f\"Accuracy:           {accuracy*100:.2f}%\")\n",
    "    print(f\"ROC AUC Score:      {roc_auc:.4f}\")\n",
    "\n",
    "    # Print macro averages\n",
    "    print(f\"\\n{'MACRO AVERAGES':-^70}\")\n",
    "    print(f\"Precision (Macro):  {precision_macro*100:.2f}%\")\n",
    "    print(f\"Recall (Macro):     {recall_macro*100:.2f}%\")\n",
    "    print(f\"F1-Score (Macro):   {f1_macro*100:.2f}%\")\n",
    "\n",
    "    # Print weighted averages\n",
    "    print(f\"\\n{'WEIGHTED AVERAGES':-^70}\")\n",
    "    print(f\"Precision (Weighted): {precision_weighted*100:.2f}%\")\n",
    "    print(f\"Recall (Weighted):    {recall_weighted*100:.2f}%\")\n",
    "    print(f\"F1-Score (Weighted):  {f1_weighted*100:.2f}%\")\n",
    "\n",
    "    # Print per-class metrics\n",
    "    print(f\"\\n{'PER-CLASS METRICS':-^70}\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"\\n{class_name} (n={support[i]}):\")\n",
    "        print(f\"  Precision:  {precision[i]*100:.2f}%\")\n",
    "        print(f\"  Recall:     {recall[i]*100:.2f}%\")\n",
    "        print(f\"  F1-Score:   {f1[i]*100:.2f}%\")\n",
    "\n",
    "    # Detailed classification report\n",
    "    print(f\"\\n{'DETAILED CLASSIFICATION REPORT':-^70}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    # Generate all plots\n",
    "    print(f\"\\n{'GENERATING VISUALIZATIONS':-^70}\")\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names)\n",
    "    plot_roc_curve(y_true, y_proba)\n",
    "    plot_metrics_comparison(precision, recall, f1, class_names)\n",
    "    plot_prediction_distribution(y_proba, y_true)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'support': support\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Models used\n",
    "    # 'distilbert/distilbert-base-uncased'\n",
    "    # 'bert-base-uncased'\n",
    "    # 'roberta-base'\n",
    "    # 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "    # 'siebert/sentiment-roberta-large-english' BEST\n",
    "    # Configuration\n",
    "    DB_PATH = '/content/gamesDB.db'              # Your database path\n",
    "    TABLE_NAME = 'reviews'              # Your table name\n",
    "    TEXT_COLUMN = 'review'         # Column with review text\n",
    "    LABEL_COLUMN = 'voted_up'    # Column with labels (0/1)\n",
    "    NUM_REVIEWS = 1000                  # Number of reviews to evaluate\n",
    "    MODEL_NAME = 'roberta-uncased'\n",
    "    BATCH_SIZE = 32\n",
    "    BALANCE_TYPE = 'oversample'\n",
    "    USE_BALANCED =  False\n",
    "\n",
    "    # Step 1: Load data from database\n",
    "    df = load_data_from_sqlite(\n",
    "        db_path=DB_PATH,\n",
    "        table_name=TABLE_NAME,\n",
    "        text_column=TEXT_COLUMN,\n",
    "        label_column=LABEL_COLUMN,\n",
    "        limit=NUM_REVIEWS\n",
    "    )\n",
    "\n",
    "    # Balance the dataset\n",
    "    if USE_BALANCED:\n",
    "      df_balanced = balance_dataset(df, BALANCE_TYPE)\n",
    "      reviews, labels = df_balanced[TEXT_COLUMN], df_balanced[LABEL_COLUMN]\n",
    "    else:\n",
    "      reviews, labels = df[TEXT_COLUMN], df[LABEL_COLUMN]\n",
    "\n",
    "    # Step 2: Make predictions\n",
    "    predictions, probabilities = predict_sentiments(\n",
    "        reviews=reviews,\n",
    "        model_name=MODEL_NAME,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Step 3: Evaluate model\n",
    "    results = evaluate_model(\n",
    "        y_true=labels,\n",
    "        y_pred=predictions,\n",
    "        y_proba=probabilities,\n",
    "        class_names=['Negative', 'Positive']\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✓ All results saved to current directory\")\n",
    "    print(f\"  - confusion_matrix.png\")\n",
    "    print(f\"  - roc_curve.png\")\n",
    "    print(f\"  - metrics_comparison.png\")\n",
    "    print(f\"  - prediction_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xf7GVj5vyQz7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
